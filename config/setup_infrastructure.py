# ğŸ—ï¸ ARCHITECTURE SETUP SCRIPT
# Configuration automatique de l'infrastructure Kafka + Spark
# Compatible Windows PowerShell

import os
import sys
import subprocess
import time
import logging
from pathlib import Path
import requests
import zipfile
import shutil
from typing import Optional, Dict, List

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('setup.log'),
        logging.StreamHandler()
    ]
)

class InfrastructureSetup:
    """Classe pour configurer l'infrastructure Kafka + Spark sur Windows"""
    
    def __init__(self):
        self.logger = logging.getLogger("InfrastructureSetup")
        self.project_dir = Path(__file__).parent.absolute()
        self.downloads_dir = self.project_dir / "downloads"
        self.kafka_dir = None
        self.spark_dir = None
        
        # Versions recommandÃ©es
        self.kafka_version = "2.13-3.6.0"
        self.spark_version = "3.5.0"
        
        # URLs de tÃ©lÃ©chargement (URLs corrigÃ©es)
        self.kafka_url = f"https://archive.apache.org/dist/kafka/3.6.0/kafka_{self.kafka_version}.tgz"
        self.spark_url = f"https://archive.apache.org/dist/spark/spark-{self.spark_version}/spark-{self.spark_version}-bin-hadoop3.tgz"
        
        # CrÃ©er dossier downloads
        self.downloads_dir.mkdir(exist_ok=True)
    
    def check_java(self) -> bool:
        """VÃ©rifie que Java est installÃ© et configurÃ©"""
        try:
            result = subprocess.run(
                ["java", "-version"], 
                capture_output=True, 
                text=True,
                shell=True
            )
            
            if result.returncode == 0:
                java_version = result.stderr.split('\n')[0]
                self.logger.info(f"âœ… Java dÃ©tectÃ©: {java_version}")
                return True
            else:
                self.logger.error("âŒ Java non trouvÃ©")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Erreur vÃ©rification Java: {e}")
            return False
    
    def install_python_dependencies(self) -> bool:
        """Installe les dÃ©pendances Python"""
        try:
            self.logger.info("ğŸ“¦ Installation des dÃ©pendances Python...")
            
            # Mise Ã  jour pip
            subprocess.run([
                sys.executable, "-m", "pip", "install", "--upgrade", "pip"
            ], check=True)
            
            # Installation requirements
            requirements_file = self.project_dir / "requirements.txt"
            if requirements_file.exists():
                subprocess.run([
                    sys.executable, "-m", "pip", "install", "-r", str(requirements_file)
                ], check=True)
                
                self.logger.info("âœ… DÃ©pendances Python installÃ©es")
                return True
            else:
                self.logger.error("âŒ Fichier requirements.txt non trouvÃ©")
                return False
                
        except subprocess.CalledProcessError as e:
            self.logger.error(f"âŒ Erreur installation dÃ©pendances: {e}")
            return False
    
    def download_and_extract(self, url: str, filename: str) -> Optional[Path]:
        """TÃ©lÃ©charge et extrait une archive"""
        try:
            file_path = self.downloads_dir / filename
            
            # TÃ©lÃ©charger si n'existe pas
            if not file_path.exists():
                self.logger.info(f"â¬‡ï¸ TÃ©lÃ©chargement {filename}...")
                
                response = requests.get(url, stream=True)
                response.raise_for_status()
                
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                self.logger.info(f"âœ… {filename} tÃ©lÃ©chargÃ©")
            else:
                self.logger.info(f"ğŸ“ {filename} dÃ©jÃ  prÃ©sent")
            
            # Extraction
            extract_dir = self.downloads_dir / filename.replace('.tgz', '').replace('.zip', '')
            
            if not extract_dir.exists():
                self.logger.info(f"ğŸ“‚ Extraction {filename}...")
                
                if filename.endswith('.tgz'):
                    import tarfile
                    with tarfile.open(file_path, 'r:gz') as tar:
                        tar.extractall(self.downloads_dir)
                elif filename.endswith('.zip'):
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        zip_ref.extractall(self.downloads_dir)
                
                self.logger.info(f"âœ… {filename} extrait")
            
            # Trouver le dossier extrait
            for item in self.downloads_dir.iterdir():
                if item.is_dir() and filename.replace('.tgz', '').replace('.zip', '') in item.name:
                    return item
            
            return extract_dir
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur tÃ©lÃ©chargement/extraction {filename}: {e}")
            return None
    
    def setup_kafka(self) -> bool:
        """Configure Apache Kafka"""
        try:
            self.logger.info("ğŸ”§ Configuration Apache Kafka...")
            
            # TÃ©lÃ©charger Kafka
            kafka_filename = f"kafka_{self.kafka_version}.tgz"
            self.kafka_dir = self.download_and_extract(self.kafka_url, kafka_filename)
            
            if not self.kafka_dir:
                return False
            
            # CrÃ©er scripts de dÃ©marrage Windows
            self.create_kafka_scripts()
            
            self.logger.info("âœ… Apache Kafka configurÃ©")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur configuration Kafka: {e}")
            return False
    
    def create_kafka_scripts(self):
        """CrÃ©e les scripts de dÃ©marrage Kafka pour Windows"""
        scripts_dir = self.project_dir / "scripts"
        scripts_dir.mkdir(exist_ok=True)
        
        # Script dÃ©marrage Zookeeper
        zookeeper_script = scripts_dir / "start_zookeeper.bat"
        zookeeper_content = f"""@echo off
echo Starting Zookeeper...
cd /d "{self.kafka_dir}"
bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties
"""
        with open(zookeeper_script, 'w') as f:
            f.write(zookeeper_content)
        
        # Script dÃ©marrage Kafka
        kafka_script = scripts_dir / "start_kafka.bat"
        kafka_content = f"""@echo off
echo Starting Kafka Server...
cd /d "{self.kafka_dir}"
bin\\windows\\kafka-server-start.bat config\\server.properties
"""
        with open(kafka_script, 'w') as f:
            f.write(kafka_content)
        
        # Script crÃ©ation topics
        topics_script = scripts_dir / "create_topics.bat"
        topics_content = f"""@echo off
echo Creating Kafka Topics...
cd /d "{self.kafka_dir}"

rem Topic pour Reddit
bin\\windows\\kafka-topics.bat --create --topic reddit_stream --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

rem Topic pour Twitter  
bin\\windows\\kafka-topics.bat --create --topic twitter_stream --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

rem Topic pour IoT Sensors
bin\\windows\\kafka-topics.bat --create --topic iot_sensors --bootstrap-server localhost:9092 --partitions 5 --replication-factor 1

rem Topic pour News Feed
bin\\windows\\kafka-topics.bat --create --topic news_feed --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1

echo Topics created successfully!
pause
"""
        with open(topics_script, 'w') as f:
            f.write(topics_content)
        
        self.logger.info("âœ… Scripts Kafka crÃ©Ã©s")
    
    def setup_spark(self) -> bool:
        """Configure Apache Spark"""
        try:
            self.logger.info("âš¡ Configuration Apache Spark...")
            
            # TÃ©lÃ©charger Spark
            spark_filename = f"spark-{self.spark_version}-bin-hadoop3.tgz"
            self.spark_dir = self.download_and_extract(self.spark_url, spark_filename)
            
            if not self.spark_dir:
                return False
            
            # Configurer variables d'environnement
            self.setup_spark_environment()
            
            # CrÃ©er scripts de dÃ©marrage
            self.create_spark_scripts()
            
            self.logger.info("âœ… Apache Spark configurÃ©")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur configuration Spark: {e}")
            return False
    
    def setup_spark_environment(self):
        """Configure les variables d'environnement Spark"""
        # CrÃ©er script d'environnement
        env_script = self.project_dir / "spark_env.bat"
        env_content = f"""@echo off
set SPARK_HOME={self.spark_dir}
set PYSPARK_PYTHON={sys.executable}
set PYSPARK_DRIVER_PYTHON={sys.executable}
set PATH=%SPARK_HOME%\\bin;%PATH%

echo Spark Environment configured
echo SPARK_HOME: %SPARK_HOME%
echo PYSPARK_PYTHON: %PYSPARK_PYTHON%
"""
        with open(env_script, 'w') as f:
            f.write(env_content)
        
        self.logger.info("âœ… Variables environnement Spark configurÃ©es")
    
    def create_spark_scripts(self):
        """CrÃ©e les scripts de dÃ©marrage Spark"""
        scripts_dir = self.project_dir / "scripts"
        scripts_dir.mkdir(exist_ok=True)
        
        # Script dÃ©marrage Spark Master
        master_script = scripts_dir / "start_spark_master.bat"
        master_content = f"""@echo off
call "{self.project_dir}\\spark_env.bat"
echo Starting Spark Master...
cd /d "%SPARK_HOME%"
bin\\spark-class.cmd org.apache.spark.deploy.master.Master
"""
        with open(master_script, 'w') as f:
            f.write(master_content)
        
        # Script dÃ©marrage Spark Worker
        worker_script = scripts_dir / "start_spark_worker.bat"
        worker_content = f"""@echo off
call "{self.project_dir}\\spark_env.bat"
echo Starting Spark Worker...
cd /d "%SPARK_HOME%"
bin\\spark-class.cmd org.apache.spark.deploy.worker.Worker spark://localhost:7077
"""
        with open(worker_script, 'w') as f:
            f.write(worker_content)
        
        self.logger.info("âœ… Scripts Spark crÃ©Ã©s")
    
    def setup_mongodb(self) -> bool:
        """Instructions pour MongoDB"""
        self.logger.info("ğŸ“„ Instructions MongoDB:")
        self.logger.info("1. TÃ©lÃ©charger MongoDB Community Server")
        self.logger.info("2. Installer avec configuration par dÃ©faut")
        self.logger.info("3. DÃ©marrer service MongoDB")
        self.logger.info("4. MongoDB sera accessible sur mongodb://localhost:27017")
        return True
    
    def create_launch_script(self):
        """CrÃ©e le script de lancement principal"""
        launch_script = self.project_dir / "launch_system.bat"
        launch_content = f"""@echo off
echo ========================================
echo   Multi-Source Analytics System
echo   Apache Kafka + Spark Streaming
echo ========================================
echo.

echo Etape 1: Demarrage Zookeeper...
start "Zookeeper" cmd /k "scripts\\start_zookeeper.bat"
timeout /t 10

echo Etape 2: Demarrage Kafka...
start "Kafka" cmd /k "scripts\\start_kafka.bat"
timeout /t 15

echo Etape 3: Creation des topics...
call scripts\\create_topics.bat

echo Etape 4: Demarrage Spark Master...
start "Spark Master" cmd /k "scripts\\start_spark_master.bat"
timeout /t 10

echo Etape 5: Demarrage Spark Worker...
start "Spark Worker" cmd /k "scripts\\start_spark_worker.bat"
timeout /t 5

echo.
echo ========================================
echo   Systeme demarre avec succes!
echo ========================================
echo.
echo Services disponibles:
echo - Kafka: localhost:9092
echo - Spark Master: localhost:7077
echo - Spark UI: http://localhost:8080
echo - MongoDB: localhost:27017
echo.
echo Pour demarrer les producteurs:
echo   python kafka_producers.py
echo.
echo Pour demarrer le consumer Spark:
echo   python spark_streaming_consumer.py
echo.
echo Pour le dashboard:
echo   streamlit run dashboard_3d_realtime.py
echo.
pause
"""
        with open(launch_script, 'w', encoding='utf-8') as f:
            f.write(launch_content)
        
        self.logger.info("âœ… Script de lancement crÃ©Ã©")
    
    def create_readme(self):
        """CrÃ©e la documentation"""
        readme_content = """# ğŸš€ Multi-Source Real-Time Analytics System

## Architecture
- **Apache Kafka**: Message streaming platform
- **Apache Spark**: Real-time stream processing
- **MongoDB**: NoSQL storage for analytics
- **Streamlit**: Interactive 3D dashboard

## ğŸ—ï¸ Installation Automatique

```bash
python setup_infrastructure.py
```

## ğŸš€ DÃ©marrage Rapide

### Option 1: Script automatique
```bash
./launch_system.bat
```

### Option 2: DÃ©marrage manuel

1. **DÃ©marrer Zookeeper**
```bash
./scripts/start_zookeeper.bat
```

2. **DÃ©marrer Kafka**
```bash
./scripts/start_kafka.bat
```

3. **CrÃ©er les topics**
```bash
./scripts/create_topics.bat
```

4. **DÃ©marrer Spark**
```bash
./scripts/start_spark_master.bat
./scripts/start_spark_worker.bat
```

5. **Lancer les producteurs**
```bash
python kafka_producers.py
```

6. **Lancer le consumer Spark**
```bash
python spark_streaming_consumer.py
```

7. **Ouvrir le dashboard**
```bash
streamlit run dashboard_3d_realtime.py
```

## ğŸ“Š Dashboard Features

- **Visualizations 3D** des trending keywords
- **Surface plots** pour Ã©volution sentiment
- **DÃ©tection d'anomalies** en temps rÃ©el
- **Analytics cross-platform** sans redÃ©marrage
- **Monitoring multi-sources** (Reddit, Twitter, IoT, News)

## ğŸ”§ Configuration

### Kafka Topics
- `reddit_stream`: DonnÃ©es Reddit (3 partitions)
- `twitter_stream`: DonnÃ©es Twitter (3 partitions)  
- `iot_sensors`: Capteurs IoT (5 partitions)
- `news_feed`: Flux d'actualitÃ©s (2 partitions)

### Services URLs
- Kafka: `localhost:9092`
- Spark UI: `http://localhost:8080`
- MongoDB: `mongodb://localhost:27017`
- Dashboard: `http://localhost:8501`

## ğŸ“ˆ Analytics Capabilities

- **Sentiment Analysis**: VADER + TextBlob + Lexicon-based
- **Keyword Trending**: FenÃªtres glissantes avec scoring
- **Anomaly Detection**: Z-score et seuils statistiques
- **Cross-Source Correlation**: Analyse inter-plateformes
- **Real-Time Visualization**: Mises Ã  jour sans interruption

## ğŸ¯ Final Project Compliance

âœ… **Multi-Source Data Ingestion**: Reddit, Twitter, IoT, News  
âœ… **Apache Kafka**: Message streaming  
âœ… **Spark Streaming**: Real-time processing  
âœ… **Text Analytics**: NLP et sentiment analysis  
âœ… **NoSQL Storage**: MongoDB intÃ©gration  
âœ… **3D Visualizations**: Plotly 3D plots  
âœ… **Real-Time Monitoring**: Dashboard temps rÃ©el  

## ğŸ”§ Troubleshooting

### Java non trouvÃ©
```bash
# Installer Java 8 ou 11
# Configurer JAVA_HOME
```

### Erreurs Kafka
```bash
# VÃ©rifier Zookeeper actif
# Ports 9092, 2181 libres
```

### Erreurs Spark
```bash
# VÃ©rifier SPARK_HOME
# Python et PySpark compatibles
```
"""
        
        readme_file = self.project_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        self.logger.info("âœ… Documentation crÃ©Ã©e")

def main():
    """Fonction principale d'installation"""
    print("ğŸš€ INSTALLATION INFRASTRUCTURE MULTI-SOURCE ANALYTICS")
    print("=" * 60)
    print("Configuration automatique:")
    print("â€¢ â˜• VÃ©rification Java")
    print("â€¢ ğŸ“¦ Installation dÃ©pendances Python")
    print("â€¢ ğŸ”§ Configuration Apache Kafka")
    print("â€¢ âš¡ Configuration Apache Spark")
    print("â€¢ ğŸ“„ CrÃ©ation scripts de dÃ©marrage")
    print("â€¢ ğŸ“š GÃ©nÃ©ration documentation")
    print()
    
    setup = InfrastructureSetup()
    
    # Ã‰tapes d'installation
    steps = [
        ("â˜• VÃ©rification Java", setup.check_java),
        ("ğŸ“¦ Installation dÃ©pendances Python", setup.install_python_dependencies),
        ("ğŸ”§ Configuration Apache Kafka", setup.setup_kafka),
        ("âš¡ Configuration Apache Spark", setup.setup_spark),
        ("ğŸ’¾ Instructions MongoDB", setup.setup_mongodb),
        ("ğŸš€ CrÃ©ation script de lancement", setup.create_launch_script),
        ("ğŸ“š GÃ©nÃ©ration documentation", setup.create_readme)
    ]
    
    success_count = 0
    
    for step_name, step_func in steps:
        try:
            print(f"\n{step_name}...")
            if step_func():
                print(f"âœ… {step_name} - SUCCESS")
                success_count += 1
            else:
                print(f"âŒ {step_name} - FAILED")
        except Exception as e:
            print(f"âŒ {step_name} - ERROR: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š INSTALLATION TERMINÃ‰E: {success_count}/{len(steps)} Ã©tapes rÃ©ussies")
    
    if success_count == len(steps):
        print("âœ… Installation complÃ¨te avec succÃ¨s!")
        print("\nğŸš€ PROCHAINES Ã‰TAPES:")
        print("1. DÃ©marrer le systÃ¨me: ./launch_system.bat")
        print("2. Ou suivre les instructions dans README.md")
        print("3. AccÃ©der au dashboard: http://localhost:8501")
    else:
        print("âš ï¸ Installation partiellement rÃ©ussie")
        print("Consulter setup.log pour plus de dÃ©tails")
    
    print("\nğŸ“š Documentation complÃ¨te disponible dans README.md")

if __name__ == "__main__":
    main()